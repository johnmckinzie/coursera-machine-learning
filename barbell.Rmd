---
title: "Barbell Lift Prediction"
author: "John McKinzie"
date: "04/24/2015"
output: html_document
---

## Summary

In this analysis, I attempted to classify correct barbell lift technique and several common mistakes using kinematic
data recorded by various fitness devices. Both Recursive Partitioning and Regression Trees and Random Forest Trees were
used. Random Forest was eventually used as the final model. The predictions for the test data ended up being correct 
when submitted.

## Dataset

The dataset used was collect by [Groupware](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight 
Lifting Exercise Dataset). This dataset is licensed under the Creative Commons license (CC BY-SA). The data was
generated by the following method:

>Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

```{r, cache=TRUE}
# load data
trainingData <- read.csv("pml-training.csv", header = TRUE, na.strings=c("","NA"))
testingData <- read.csv("pml-testing.csv", header = TRUE, na.strings=c("","NA"))
```

```{r, echo=FALSE}
trainingDataDim <- dim(trainingData)
```

The testing data contains `r trainingDataDim[1]` observations of `r trainingDataDim[2]` variables. The columns that 
contained an empty or `NA` observation were removed. Also, the first seven variables, which were metadata about the 
observation, were also removed.

```{r, echo=FALSE, cache=TRUE}
library(caret); library(rpart); library(randomForest)
```

```{r, cache=TRUE}
# remove metadata and empty/NA columns
trainingData <- trainingData[, -(1:7)]
trainingData <- trainingData[, colSums(is.na(trainingData)) == 0]
testingData <- testingData[, -(1:7)]
testingData <- testingData[, colSums(is.na(testingData)) == 0]
```

```{r, echo=FALSE}
trainingDataDim <- dim(trainingData)
```

After cleaning up the data, there are now `r trainingDataDim[2]` variables. Next, I created testing and training
partitions from the training data file. Because of the size of the dataset, a 60/40 split was use for training and 
testing partitions.

```{r, cache=TRUE}
# create training and test datasets from training data
set.seed(9991)
inTrain <- createDataPartition(y=trainingData$classe, p=.60, list=FALSE)
trainingSet <- trainingData[inTrain,]
testingSet <- trainingData[-inTrain,]
```

## Recursive Partitioning and Regression Trees

The first model used is a Recursive Partitioning and Regression Trees.

```{r, cache=TRUE}
# rpart
modRPart <- train(classe ~ ., method="rpart", data=trainingSet)
predRPart <- predict(modRPart, newdata=testingSet)
confRPart <- confusionMatrix(predRPart, testingSet$classe)
print(confRPart)
```

```{r, echo=FALSE, echo=FALSE}
plot(confRPart[[2]], main="Confusion Matrix for the Recursive Partitioning and Regression Tree")
```

When cross-validating against our test dataset, we can see that the model had an accuracy of 
`r paste0(formatC(100 * confRPart$overall[[1]], digits = 2), "%")`. This was less than ideal.

## Random Forest Trees

Since the Recursive Partitioning and Regression Tree did not produce accurate classifications, a Random Forest Tree was
used. 

```{r, cache=TRUE}
# random forest
modRF <- randomForest(classe ~ ., data=trainingData)
print(modRF)
```

The out-of-bag (OOB) error, which is similar to out-of-sample error for when bagging, for this model was 0.27%. This led
me to believe this model would be very accurate when applied to our test partition.

```{r, cache=TRUE}
predRF <- predict(modRF, newdata = testingSet)
confRF <- confusionMatrix(predRF, testingSet$classe)
```

```{r, echo=FALSE, cache=TRUE}
print(confRF)
plot(confRF[[2]], main="Confusion Matrix for the Random Forest Tree")
```

The confusion matrix for the model and test data showed the model was 100% accurate in classifying the test 
partition of the test data.

## Random Forest Trees with PCA

I wanted to look at one more model before attempting to classify the test data. Here I used a Random Forest Tree but 
with PCA preprocessing.

```{r, cache=TRUE}
# random forest w/ pca
modRFPCA <- randomForest(classe ~ ., data=trainingData, preProcess="pca")
print(modRFPCA)
```

In this case, using PCA actually gave a slightly higher OOB error rate of 29%.

```{r, cache=TRUE}
predRFPCA <- predict(modRFPCA, newdata = testingSet)
confRFPCA <- confusionMatrix(predRFPCA, testingSet$classe)
```

```{r, echo=FALSE, cache=TRUE}
print(confRFPCA)
plot(confRFPCA[[2]], main="Confusion Matrix for the Random Forest Tree w/ PCA")
```

While the OOB error rate was higher, the accuracy of this model was also 100% when apply to the test partition.

## Test Data

The Random Forest Trees was used to determine the class of each of the 20 observations in the test data, since it had
the lower OOB error rate. Below are the results of model:

```{r, cache=TRUE}
predTestRFP <- predict(modRFPCA, newdata=testingData)
print(predTestRFP)
```

## Citations

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz3YLEDMowx
